
import { NavItem, ResearchInterest, VisitedPlace, Publication, NewsItem, BlogPost } from './types';

export const NAV_ITEMS: NavItem[] = [
  { label: 'Home', path: '/' },
  { label: 'Blogs', path: '/blog' },
  { label: 'Publications', path: '/publications' },
  { label: 'Travel Gallery', path: '/travel' },
];

export const RESEARCH_INTERESTS: ResearchInterest[] = [
  {
    title: "Robust Alignment",
    description:
      "Studying whether aligned behavior remains stable beyond training conditions. This includes both benign distribution shifts and adversarial settings such as jailbreaks or strategic prompt manipulation. A model may appear aligned on training-like data yet fail when faced with novel inputs or pressure that exploits weaknesses in its learned constraints. This direction focuses on understanding what makes alignment stable across regimes, and how robustness can be achieved on narrow supervision.",
    period: "2025.6 - until now",
    colorTheme: "bg-anthropic-leaf/20 border-anthropic-leaf/30"
  },
  {
    title: "Evaluation and Monitoring",
    description:
      "Studying how alignment failures can remain hidden under existing evaluations and monitoring signals. Models may appear safe or aligned according to standard metrics while exhibiting subtle or delayed failures in real use. This direction focuses on identifying blind spots in evaluation and monitoring, and understanding how risk can emerge or evolve beyond what current oversight mechanisms capture.",
    period: "2025.6 - until now",
    colorTheme: "bg-anthropic-stone/50 border-anthropic-stone"
  },
  {
    title: "Scalable Oversight",
    description:
      "Focusing on how supervision can function when direct human judgment is no longer sufficient for agent outputs. As models handle more complex reasoning and long-horizon tasks, human evaluators may not reliably assess correctness or safety. This direction examines how oversight can be structured through partial supervision, indirect signals, and decomposition of evaluation, rather than human judgement relying on the outputs.",
    period: "2025.12 - until now",
    colorTheme: "bg-anthropic-mist/40 border-anthropic-mist/60"
  },
  {
    title: "Super Alignment",
    description:
      "Studying how alignment can be maintained when models surpass human ability to directly supervise their reasoning or outcomes. As models become more capable, alignment must rely on indirect signals, weaker forms of oversight, or objectives specified under deep uncertainty. This direction focuses on understanding the failure modes that emerge in such settings and what it means for alignment to remain meaningful when human judgment is no longer a reliable reference.",
    period: "2025.12 - until now",
    colorTheme: "bg-anthropic-sand/40 border-anthropic-sand/60"
  }
];

export const PUBLICATIONS: Publication[] = [
  {
    id: 'outcome-aware-safety',
    title: "Read the Scene, Not the Script: Outcome-Aware Safety for LLMs",
    authors: ["Rui Wu", "Yihao Quan", "Zeru Shi", "Zhenting Wang", "Yanshu Li", "Ruixiang Tang"],
    venue: "NeurIPS 2025 ResponsibleFM Workshop",
    year: 2025,
    link: "https://arxiv.org/abs/2510.04320",
    github: "https://github.com/RuiWu1123/Outcome-Aware-Safety-for-LLMs",
    tags: ["Generalized Safety Alignment"],
    preprint: true
  }
];

export const NEWS_ITEMS: NewsItem[] = [
  {
    date: "2025/11/28",
    content: "Completed a 4-day fancy trip to Iceland! See my \"travel gallery\"!",
    link: "#/travel"
  },
  {
    date: "2025/11/8",
    content: "My paper \"Read the Scene, Not the Script: Outcome-Aware Safety for LLMs \" is accepted by NeurIPS 2025 ResponsibleFM Workshop!",
    link: "https://arxiv.org/abs/2510.04320"
  }
];

export const BLOG_POSTS: BlogPost[] = [
  {
    id: "my-research-river",
    title: "Why these Research Topics? Sharing my long-term vision",
    date: "2025/12/15",
    generator: "ChatGPT5.1",
    content: `
*This blog was entirely generated by ChatGPT5.1 without any human revision. I just talked with GPT and then let it summarize the chat, and I basically agree with the details of the article completion.*

When I look at my current research directions, they do not feel like a predefined taxonomy or a cleanly partitioned research agenda. They emerged gradually from experiments, implementation choices, and repeated design decisions that kept pointing toward the same underlying tension: how to align increasingly capable models in a way that remains stable outside the narrow conditions under which alignment is typically trained and evaluated. Over time, these efforts began to form a structure that only became clear in retrospect. What I describe below is not a fixed roadmap, but a snapshot of how my thinking about alignment has organized itself so far.

The first and central direction is Robust Alignment. This line of work asks whether aligned behavior remains stable beyond training conditions, including both benign distribution shifts and adversarial settings such as jailbreaks or strategic prompt manipulation. Many models appear aligned when evaluated on training-like data or standard benchmarks, yet fail when exposed to novel inputs or pressure that exploits weaknesses in their learned constraints. Robust alignment treats these failures not as isolated bugs, but as evidence that alignment itself lacks stability. The focus is on understanding what makes aligned behavior persist across regimes, and how robustness can be achieved without relying on brittle pattern matching or narrowly specified supervision.

As alignment methods are pushed toward robustness, however, a second question naturally arises: how do we know when alignment has actually failed? This motivates my interest in Evaluation and Monitoring. Models may appear safe or aligned according to existing metrics while exhibiting subtle, delayed, or context-dependent failures in real use. This direction studies how alignment failures can remain hidden under current evaluation and monitoring signals, where blind spots arise, and how risk can emerge or evolve beyond what oversight mechanisms are designed to capture. Rather than assuming that better metrics will solve the problem, this work treats evaluation itself as a potential source of miscalibration.

The difficulty of evaluation becomes even more pronounced as models handle increasingly complex reasoning and long-horizon tasks. This leads to Scalable Oversight, which focuses on how supervision can function when direct human judgment is no longer sufficient. In such settings, oversight must rely on partial supervision, indirect signals, or decomposed evaluation structures. This direction examines how alignment methods behave under these constraints and which forms of oversight remain effective when perfect evaluation is unavailable. Instead of assuming that supervision can simply be scaled up, scalable oversight studies where and why supervision breaks down as model capability increases.

Pushing these questions further leads to Super Alignment, which addresses alignment in regimes where models surpass human ability to directly supervise their reasoning or outcomes. In these settings, alignment must rely on indirect objectives, weaker forms of oversight, or goals specified under deep uncertainty. This direction focuses on understanding the failure modes that emerge when human judgment is no longer a reliable reference point, and on clarifying what it even means for alignment to remain meaningful under such conditions. Super alignment is not treated as a distant or abstract problem, but as the limiting case implied by the trends observed in more practical alignment settings.

Seen together, these directions form a coherent progression. Robust alignment asks whether aligned behavior is stable outside training conditions. Evaluation and monitoring question whether we can reliably detect when that stability breaks. Scalable oversight examines whether alignment can be maintained as supervision weakens. Super alignment confronts the regime in which human judgment itself can no longer anchor alignment. Together, they reflect a shift from treating alignment as a collection of techniques to treating it as a scientific problem about stability, supervision, and epistemic reliability. This perspective continues to guide how I choose problems and design experiments, even as the specific methods evolve over time.
    `
  },
  {
    id: "alignment-science-safety",
    title: "Alignment Science and Safety Alignment: A Perspective",
    date: "2025/12/01",
    generator: "ChatGPT5.1",
    content: `
*This blog was entirely generated by ChatGPT5.1 without any human revision. I only used three sentences to express my viewpoint, and I basically agree with the details of the article completion.*

When people talk about AI alignment, it is often implicitly equated with safety. Alignment is described as preventing harm, enforcing refusals, or making models avoid dangerous behaviors. While this framing is understandable, it is incomplete. In a broader sense, alignment is about how models attach to goals, values, and identities, and how reliably they can be guided across different contexts. From this perspective, safety alignment is not the definition of alignment, but one specific and practically important instance within a larger scientific problem. Safety happens to be the place where misalignment is easiest to observe, where signals are clearer, and where failures carry obvious consequences.

Seen this way, safety alignment plays a dual role. On the one hand, it is an urgent applied problem with real stakes. On the other hand, it functions as a diagnostic setting for alignment more generally. Many alignment pathologies first become visible in safety tasks, such as unstable refusal behavior, inconsistent goal adherence, or shallow compliance that collapses under distribution shift. These issues are not unique to safety. They reflect deeper properties of how models represent goals, switch between modes, and manage competing objectives. Safety alignment therefore matters not because it is special in kind, but because it exposes alignment limits earlier and more clearly than most other domains.

However, as the field has matured, the role of safety alignment has gradually shifted. What was once a space for probing fundamental alignment questions has increasingly become a competitive engineering arena. Much of current work focuses on incremental performance gains through data engineering, prompt optimization, and post training frameworks. New datasets are constructed, reward models refined, and pipelines tuned to extract small improvements on established benchmarks. This mirrors the evolution seen in areas like mathematical reasoning or code generation, where progress is often measured by leaderboard movement rather than conceptual understanding. While this work is not without value, it has also narrowed the scope of inquiry. Safety alignment risks becoming a points driven subfield, where success is defined by metric gains rather than insight into why alignment works or fails.

This shift has consequences. Engineering focused safety work can improve surface behavior while leaving underlying alignment assumptions unexamined. Models may learn how to pass evaluations without developing stable goal representations or robust value adherence. As a result, we risk mistaking short term behavioral fixes for genuine alignment progress. When alignment research is dominated by incremental engineering, the field may advance in capability while stagnating in understanding. The gap between what models do and why they do it remains largely unaddressed.

For this reason, I believe there is a growing need to rebalance attention toward Alignment Science. Alignment science is concerned with the foundational questions that sit beneath applied safety work. It asks how goals are represented internally, how value conflicts are resolved, how identities remain stable across tasks, and how supervision signals shape long term behavior. These questions are harder to benchmark and slower to reward, but they are essential if alignment methods are to remain reliable as models scale. Without a stronger scientific understanding of alignment mechanisms, safety alignment risks becoming an increasingly fragile patchwork of techniques.

Safety alignment will continue to be important, and it should. But its greatest value may lie in how it informs alignment science rather than how it competes on leaderboards. By treating safety as a lens rather than an endpoint, we can use it to study alignment at a deeper level. The hope is that more researchers will look beyond immediate metric gains and invest in understanding the principles that govern alignment across domains. Only then can we expect alignment methods to remain stable, interpretable, and effective as AI systems grow more capable.
    `
  }
];

export const SOCIAL_LINKS = {
  scholar: "https://scholar.google.com/citations?user=M1FovLwAAAAJ&hl=en"
};

export const VISITED_PLACES: VisitedPlace[] = [
  {
    id: 'iceland',
    name: 'Iceland',
    coordinates: [-19.0208, 64.9631],
    date: '2025/11/24-2025/11/28',
    description: 'Blue Lagoon, Golden Circle, Sn√¶fellsnes Peninsula, Aurora Borealis and more.',
    visitCount: 1,
    images: [
      '/travel_pictures/iceland/1b59c304f6cbfe18538f82088182ec89.jpg',
      '/travel_pictures/iceland/311fcb634aaa63150e69c5a2a695ba76.jpg',
      '/travel_pictures/iceland/3cabcbf41a1b08f2f7fb4f31972a8475.jpg',
      '/travel_pictures/iceland/4c0fb2d709e2ade2ef61d846e8a0f32d.jpg',
      '/travel_pictures/iceland/4cc8ce3d20062c0b17e23c76484c8cd9.jpg',
      '/travel_pictures/iceland/e28b49b22857f989dd9833e0d7ed2e6c.jpg',
      '/travel_pictures/iceland/e355376cc0f8c47518a5f4ebee5ef95f.jpg',
      '/travel_pictures/iceland/ecac61d106da06b3c6e843c11e124341.jpg',
      '/travel_pictures/iceland/f8ab3bf7b0a4d53df4f0e63cc6c5107d.jpg'
    ]
  }
];
