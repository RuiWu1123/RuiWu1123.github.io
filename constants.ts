
import { NavItem, ResearchInterest, VisitedPlace, Publication, NewsItem, BlogPost } from './types';

export const NAV_ITEMS: NavItem[] = [
  { label: 'Home', path: '/' },
  { label: 'Blogs', path: '/blog' },
  { label: 'Publications', path: '/publications' },
  { label: 'Travel Gallery', path: '/travel' },
];

export const RESEARCH_INTERESTS: ResearchInterest[] = [
  {
    title: "Generalized Safety Alignment",
    description:
      "Studying whether safety-aligned behavior truly generalizes beyond the specific prompts, tasks, or distributions seen during training. Many safety methods perform well on benchmarked scenarios but fail under slight distribution shifts or novel compositions of risk. This direction focuses on understanding what it means for safety alignment to generalize, how brittle or robust different alignment strategies are, and which inductive biases encourage safer behavior in unfamiliar settings.",
    period: "2025.6 - until now",
    colorTheme: "bg-anthropic-leaf/20 border-anthropic-leaf/30"
  },
  {
    title: "Fine-Grained Safety Alignment",
    description:
      "Studying cases where a model shows clear internal signals about risk but still produces unsafe behavior because the reward during RLHF was imperfect. This direction focuses on how gaps form between internal representations and final decoding, and how these signals can help improve safety as post-alignment after the RLHF. The goal is to build post-alignment methods that remain reliable even when the reward design cannot fully capture the desired behavior.",
    period: "2025.6 - until now",
    colorTheme: "bg-anthropic-stone/50 border-anthropic-stone"
  },
  {
    title: "Scalable Oversight",
    description:
      "Focusing on how supervision and alignment can function when direct human judgment is no longer sufficient. As models handle more complex reasoning and long-horizon tasks, human evaluators may not reliably assess correctness or safety. This direction examines how oversight can be structured through partial supervision, indirect signals, and decomposition of evaluation, rather than relying on full ground-truth labels.",
    period: "2025.12 - until now",
    colorTheme: "bg-anthropic-mist/40 border-anthropic-mist/60"
  },
  {
    title: "Evaluation of Supervision",
    description:
      "Focusing on evaluating supervision themselves rather than the models trained on them. Many alignment methods assume that labels, rewards, or evaluators reliably reflect the intended objectives, yet supervision can be incomplete, inconsistent, or systematically biased. This direction studies how well supervision captures the behaviors it is meant to control, where it fails to reflect real concerns, and how these failures propagate into training and evaluation outcomes.",
    period: "2025.12 - until now",
    colorTheme: "bg-anthropic-sand/40 border-anthropic-sand/60"
  }
];

export const PUBLICATIONS: Publication[] = [
  {
    id: 'outcome-aware-safety',
    title: "Read the Scene, Not the Script: Outcome-Aware Safety for LLMs",
    authors: ["Rui Wu", "Yihao Quan", "Zeru Shi", "Zhenting Wang", "Yanshu Li", "Ruixiang Tang"],
    venue: "NeurIPS 2025 ResponsibleFM Workshop",
    year: 2025,
    link: "https://arxiv.org/abs/2510.04320",
    github: "https://github.com/RuiWu1123/Outcome-Aware-Safety-for-LLMs",
    tags: ["Generalized Safety Alignment"],
    preprint: true
  }
];

export const NEWS_ITEMS: NewsItem[] = [
  {
    date: "2025/11/28",
    content: "Completed a 4-day fancy trip to Iceland! See my \"travel gallery\"!",
    link: "#/travel"
  },
  {
    date: "2025/11/8",
    content: "My paper \"Read the Scene, Not the Script: Outcome-Aware Safety for LLMs \" is accepted by NeurIPS 2025 ResponsibleFM Workshop!",
    link: "https://arxiv.org/abs/2510.04320"
  }
];

export const BLOG_POSTS: BlogPost[] = [
  {
    id: "my-research-river",
    title: "Why these Research Topics? Sharing my long-term vision",
    date: "2025/12/05, last updated 2025/12/15",
    generator: "ChatGPT5.1",
    content: `
*This blog was entirely generated by ChatGPT5.1 without any human revision. I just talked with GPT and then let it summarize the chat, and I basically agree with the details of the article completion.*

When I look at my current research directions, they do not feel like a set of predefined categories. They emerged gradually from experiments, implementation choices, and repeated design decisions that kept pointing toward the same underlying tension: how to align increasingly capable models in a way that remains stable beyond narrow benchmarks. Over time, these efforts formed a structure that made sense only in retrospect. What I describe below is not a fixed roadmap, but a snapshot of how my thinking about alignment has organized itself so far.

The first direction is Generalized Safety Alignment, which focuses on developing methods that encourage safety behaviors to generalize beyond specific prompts or training distributions. A central idea here is to use reasoning as a mechanism for generalization rather than treating safety as a collection of surface level rules. For example, instead of training models to refuse specific classes of requests, this direction explores how safety reasoning can be incorporated so that models learn to anticipate harmful consequences implied by their own outputs. By framing safety as a reasoning problem that requires projecting downstream effects rather than matching patterns, this approach aims to align safety behavior with the same capabilities that support general task generalization.

Closely related is Fine Grained Safety Alignment, which operates at a more detailed level of the model’s internal decision process. Rather than relying solely on final outputs or binary refusal signals, this direction leverages inconsistencies between internal signals and outward behavior as alignment opportunities. In practice, this means using signals such as internal risk representations or intermediate activations to guide post alignment, especially in cases where reward signals from RLHF are coarse or imperfect. The goal is to align models not only at the behavioral level, but across different internal stages, so that safety relevant information expressed inside the model is reflected more consistently in its final decisions.

Together, these two directions represent a method oriented view of safety alignment. They aim to improve safety behavior directly. One focuses on strengthening generalization through reasoning, while the other focuses on tightening alignment at finer internal granularity. As these methods developed, however, a broader question began to surface. If safety alignment increasingly depends on supervision signals, whether they come from reasoning traces, internal indicators, or reward models, how reliable are those signals as models continue to scale.

This question motivates my interest in Scalable Oversight. As models become more capable, direct human evaluation becomes less practical. Oversight must often rely on partial judgments, decomposed tasks, or approximate evaluators. Scalable oversight studies how alignment methods behave under these constraints and what forms of supervision remain effective when perfect evaluation is unavailable. Rather than proposing a single solution, this direction examines how oversight structures interact with model capability and where stability can or cannot be maintained as supervision becomes inherently limited.

Closely connected is Evaluation of Supervision, which shifts attention to the supervision signals themselves. Instead of assuming that labels, rewards, or evaluators faithfully encode desired objectives, this direction treats supervision as an object of evaluation. It asks how well supervision captures the behaviors it is meant to guide, where it introduces blind spots, and how its limitations propagate into both training and downstream assessment. Evaluating supervision is a necessary step before relying on it to align more capable models, especially when misaligned signals can silently shape behavior in unintended ways.

Seen in sequence, these directions form a coherent progression. Generalized and fine grained safety alignment focus on developing concrete methods for safer behavior. Scalable oversight examines whether those methods can hold up as supervision weakens. Evaluation of supervision questions whether the supervision we rely on is valid in the first place. Together, they reflect a shift from treating alignment as a collection of techniques to treating it as a scientific problem about supervision, incentives, and reliability. This perspective continues to guide how I choose problems and design experiments, even as the details evolve over time.
    `
  },
  {
    id: "alignment-science-safety",
    title: "Alignment Science and Safety Alignment: A Perspective",
    date: "2025/12/01",
    generator: "ChatGPT5.1",
    content: `
*This blog was entirely generated by ChatGPT5.1 without any human revision. I only used three sentences to express my viewpoint, and I basically agree with the details of the article completion.*

When people talk about AI alignment, it is often implicitly equated with safety. Alignment is described as preventing harm, enforcing refusals, or making models avoid dangerous behaviors. While this framing is understandable, it is incomplete. In a broader sense, alignment is about how models attach to goals, values, and identities, and how reliably they can be guided across different contexts. From this perspective, safety alignment is not the definition of alignment, but one specific and practically important instance within a larger scientific problem. Safety happens to be the place where misalignment is easiest to observe, where signals are clearer, and where failures carry obvious consequences.

Seen this way, safety alignment plays a dual role. On the one hand, it is an urgent applied problem with real stakes. On the other hand, it functions as a diagnostic setting for alignment more generally. Many alignment pathologies first become visible in safety tasks, such as unstable refusal behavior, inconsistent goal adherence, or shallow compliance that collapses under distribution shift. These issues are not unique to safety. They reflect deeper properties of how models represent goals, switch between modes, and manage competing objectives. Safety alignment therefore matters not because it is special in kind, but because it exposes alignment limits earlier and more clearly than most other domains.

However, as the field has matured, the role of safety alignment has gradually shifted. What was once a space for probing fundamental alignment questions has increasingly become a competitive engineering arena. Much of current work focuses on incremental performance gains through data engineering, prompt optimization, and post training frameworks. New datasets are constructed, reward models refined, and pipelines tuned to extract small improvements on established benchmarks. This mirrors the evolution seen in areas like mathematical reasoning or code generation, where progress is often measured by leaderboard movement rather than conceptual understanding. While this work is not without value, it has also narrowed the scope of inquiry. Safety alignment risks becoming a points driven subfield, where success is defined by metric gains rather than insight into why alignment works or fails.

This shift has consequences. Engineering focused safety work can improve surface behavior while leaving underlying alignment assumptions unexamined. Models may learn how to pass evaluations without developing stable goal representations or robust value adherence. As a result, we risk mistaking short term behavioral fixes for genuine alignment progress. When alignment research is dominated by incremental engineering, the field may advance in capability while stagnating in understanding. The gap between what models do and why they do it remains largely unaddressed.

For this reason, I believe there is a growing need to rebalance attention toward Alignment Science. Alignment science is concerned with the foundational questions that sit beneath applied safety work. It asks how goals are represented internally, how value conflicts are resolved, how identities remain stable across tasks, and how supervision signals shape long term behavior. These questions are harder to benchmark and slower to reward, but they are essential if alignment methods are to remain reliable as models scale. Without a stronger scientific understanding of alignment mechanisms, safety alignment risks becoming an increasingly fragile patchwork of techniques.

Safety alignment will continue to be important, and it should. But its greatest value may lie in how it informs alignment science rather than how it competes on leaderboards. By treating safety as a lens rather than an endpoint, we can use it to study alignment at a deeper level. The hope is that more researchers will look beyond immediate metric gains and invest in understanding the principles that govern alignment across domains. Only then can we expect alignment methods to remain stable, interpretable, and effective as AI systems grow more capable.
    `
  }
];

export const SOCIAL_LINKS = {
  scholar: "https://scholar.google.com/citations?user=M1FovLwAAAAJ&hl=en"
};

export const VISITED_PLACES: VisitedPlace[] = [
  {
    id: 'iceland',
    name: 'Iceland',
    coordinates: [-19.0208, 64.9631],
    date: '2025/11/24-2025/11/28',
    description: 'Blue Lagoon, Golden Circle, Snæfellsnes Peninsula, Aurora Borealis and more.',
    visitCount: 1,
    images: [
      '/travel_pictures/iceland/1b59c304f6cbfe18538f82088182ec89.jpg',
      '/travel_pictures/iceland/311fcb634aaa63150e69c5a2a695ba76.jpg',
      '/travel_pictures/iceland/3cabcbf41a1b08f2f7fb4f31972a8475.jpg',
      '/travel_pictures/iceland/4c0fb2d709e2ade2ef61d846e8a0f32d.jpg',
      '/travel_pictures/iceland/4cc8ce3d20062c0b17e23c76484c8cd9.jpg',
      '/travel_pictures/iceland/e28b49b22857f989dd9833e0d7ed2e6c.jpg',
      '/travel_pictures/iceland/e355376cc0f8c47518a5f4ebee5ef95f.jpg',
      '/travel_pictures/iceland/ecac61d106da06b3c6e843c11e124341.jpg',
      '/travel_pictures/iceland/f8ab3bf7b0a4d53df4f0e63cc6c5107d.jpg'
    ]
  }
];
