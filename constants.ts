
import { NavItem, ResearchInterest, VisitedPlace, Publication, NewsItem, BlogPost } from './types';

export const NAV_ITEMS: NavItem[] = [
  { label: 'Home', path: '/' },
  { label: 'Blogs', path: '/blog' },
  { label: 'Publications', path: '/publications' },
  { label: 'Travel Gallery', path: '/travel' },
];

export const RESEARCH_INTERESTS: ResearchInterest[] = [
  {
    title: "Scalable Oversight",
    description: "Investigating techniques to align superhuman AI systems where human supervision becomes scarce or unreliable. Focusing on weak-to-strong generalization and scalable oversight.",
    period: "2025.6 - until now",
    colorTheme: "bg-anthropic-stone/50 border-anthropic-stone"
  },
  {
    title: "Reasoning to Align",
    description: "Exploring how chain-of-thought and internal reasoning processes can be steered towards safe and aligned outcomes, ensuring transparency in model decision-making.",
    period: "2025.6 - until now",
    colorTheme: "bg-anthropic-leaf/20 border-anthropic-leaf/30"
  }
];

export const PUBLICATIONS: Publication[] = [
  {
    id: 'outcome-aware-safety',
    title: "Read the Scene, Not the Script: Outcome-Aware Safety for LLMs Reasoning",
    authors: ["Rui Wu", "Yihao Quan", "Zeru Shi", "Zhenting Wang", "Yanshu Li", "Ruixiang Tang"],
    venue: "NeurIPS 2025 ResponsibleFM Workshop",
    year: 2025,
    link: "https://arxiv.org/abs/2510.04320",
    github: "https://github.com/RuiWu1123/Outcome-Aware-Safety-for-LLMs",
    tags: ["Reasoning to Align"],
    preprint: true
  }
];

export const NEWS_ITEMS: NewsItem[] = [
  {
    date: "2025/11/28",
    content: "Completed a 4-day fancy trip to Iceland! See my \"travel gallery\"!",
    link: "#/travel"
  },
  {
    date: "2025/11/8",
    content: "My paper \"Read the Scene, Not the Script: Outcome-Aware Safety for LLMs Reasoning\" to Align is accepted by NeurIPS 2025 ResponsibleFM Workshop!",
    link: "https://arxiv.org/abs/2510.04320"
  }
];

export const BLOG_POSTS: BlogPost[] = [
  {
    id: "alignment-science-safety",
    title: "Alignment Science in AI and Its Relationship with Safety Alignment",
    date: "Nov 2025",
    generator: "ChatGPT5.1",
    content: `
*This blog was entirely generated by chatGPT5.1 without any human revision. I only used three sentences to express my viewpoint, and I basically agree with the details of the article completion.*

When people talk about AI alignment, they often imagine the classic idea of forcing a model to follow human values or preventing it from doing harmful things. This view is narrow and does not capture what alignment means for modern AI systems. In my own understanding, alignment is about making models easy to guide. A model should be able to enter different downstream environments and quickly adapt to the identity, goals, and values that the user needs at that moment. The core idea is not to force a single global value system into the model but to make the model flexible enough so that it can be aligned on demand. This perspective treats alignment as a general scientific problem. It is about how a model represents goals internally, how it switches between modes, and how stable those switches are during generation and reasoning.

If we think in this broader way, safety alignment becomes one special case inside the larger picture. It is a subset with clear motivation. It is also easier to study compared to many other open ended tasks. Safety tasks usually have more concrete signals and more stable reward definitions. For example, identifying a harmful question or rejecting a dangerous instruction gives us clearer feedback than artistic taste, personality, or many open domain tasks where there is no single right answer. This is why safety has grown into such a large research field. It gives us useful data and allows us to build training loops that are grounded in observable behavior. Many alignment challenges, such as value shift, inconsistent refusal, or unstable identity, show up first in safety experiments. These problems then give us clues about how alignment works in general.

At the same time, safety cannot be isolated from the larger structure of alignment science. A model that fails to maintain consistent identity across tasks will also fail in its safety behaviors. A model that cannot hold stable goals across long reasoning traces is unlikely to be robust in high risk settings. Safety becomes a stress test that reveals deeper alignment limits. It pushes us to explore how models form internal preferences, how they manage conflicting instructions, and how they track consequences. These mechanisms matter across all applications, not only dangerous ones. A model that understands consequences and maintains self consistency will provide better reasoning even in simple everyday tasks.

Thinking about alignment in this unified way helps us move past the old idea of alignment as a single locked in value set. Instead, alignment is a science of how models attach to goals. It covers representation learning, preference shaping, identity stability, chain of thought structure, and many other aspects of model behavior. Safety sits inside this space as a focused area where the signals happen to be strong and the errors more visible. By studying safety, we learn methods and principles that transfer to other domains, such as personalization, controllability, and long horizon reasoning. The hope is that through this broader view we can build models that remain understandable and steerable no matter what new tasks the future brings.

AI alignment is not only about preventing harm. It is about making models adaptable in a controlled and predictable way. When we see safety as part of this larger scientific map, we gain a clearer sense of direction. We are not simply patching refusal logic or filtering content. We are studying how goal adherence forms inside a model and how we can shape it. This brings alignment out of the narrow “safety only” frame and turns it into a foundation for general AI behavior. It also opens space for techniques that unify alignment across different domains instead of building isolated systems for each task. Safety remains important and urgent, but it becomes more meaningful when connected to the wider framework of alignment science.
    `
  }
];

export const SOCIAL_LINKS = {
  scholar: "https://scholar.google.com/citations?user=M1FovLwAAAAJ&hl=en"
};

export const VISITED_PLACES: VisitedPlace[] = [
  {
    id: 'iceland',
    name: 'Iceland',
    coordinates: [-19.0208, 64.9631],
    date: 'Nov 2025',
    description: 'A 4-day fancy trip exploring the land of fire and ice. Witnessed the northern lights, walked on black sand beaches, and visited the Blue Lagoon.',
    visitCount: 1,
    images: [
      '/travel_pictures/iceland/1b59c304f6cbfe18538f82088182ec89.jpg',
      '/travel_pictures/iceland/311fcb634aaa63150e69c5a2a695ba76.jpg',
      '/travel_pictures/iceland/3cabcbf41a1b08f2f7fb4f31972a8475.jpg',
      '/travel_pictures/iceland/4c0fb2d709e2ade2ef61d846e8a0f32d.jpg',
      '/travel_pictures/iceland/4cc8ce3d20062c0b17e23c76484c8cd9.jpg',
      '/travel_pictures/iceland/e28b49b22857f989dd9833e0d7ed2e6c.jpg',
      '/travel_pictures/iceland/e355376cc0f8c47518a5f4ebee5ef95f.jpg',
      '/travel_pictures/iceland/ecac61d106da06b3c6e843c11e124341.jpg',
      '/travel_pictures/iceland/f8ab3bf7b0a4d53df4f0e63cc6c5107d.jpg'
    ]
  }
];
