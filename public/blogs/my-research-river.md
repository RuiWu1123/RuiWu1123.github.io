---
title: "Why these Research Topics? Sharing my long-term vision"
date: "2025/12/15"
generator: "ChatGPT5.1"
---

*This blog was entirely generated by ChatGPT5.1 without any human revision. I just talked with GPT and then let it summarize the chat, and I basically agree with the details of the article completion.*

When I look at my current research directions, they do not feel like a predefined taxonomy or a cleanly partitioned research agenda. They emerged gradually from experiments, implementation choices, and repeated design decisions that kept pointing toward the same underlying tension: how to align increasingly capable models in a way that remains stable outside the narrow conditions under which alignment is typically trained and evaluated. Over time, these efforts began to form a structure that only became clear in retrospect. What I describe below is not a fixed roadmap, but a snapshot of how my thinking about alignment has organized itself so far.

The first and central direction is Robust Alignment. This line of work asks whether aligned behavior remains stable beyond training conditions, including both benign distribution shifts and adversarial settings such as jailbreaks or strategic prompt manipulation. Many models appear aligned when evaluated on training-like data or standard benchmarks, yet fail when exposed to novel inputs or pressure that exploits weaknesses in their learned constraints. Robust alignment treats these failures not as isolated bugs, but as evidence that alignment itself lacks stability. The focus is on understanding what makes aligned behavior persist across regimes, and how robustness can be achieved without relying on brittle pattern matching or narrowly specified supervision.

As alignment methods are pushed toward robustness, however, a second question naturally arises: how do we know when alignment has actually failed? This motivates my interest in Evaluation and Monitoring. Models may appear safe or aligned according to existing metrics while exhibiting subtle, delayed, or context-dependent failures in real use. This direction studies how alignment failures can remain hidden under current evaluation and monitoring signals, where blind spots arise, and how risk can emerge or evolve beyond what oversight mechanisms are designed to capture. Rather than assuming that better metrics will solve the problem, this work treats evaluation itself as a potential source of miscalibration.

The difficulty of evaluation becomes even more pronounced as models handle increasingly complex reasoning and long-horizon tasks. This leads to Scalable Oversight, which focuses on how supervision can function when direct human judgment is no longer sufficient. In such settings, oversight must rely on partial supervision, indirect signals, or decomposed evaluation structures. This direction examines how alignment methods behave under these constraints and which forms of oversight remain effective when perfect evaluation is unavailable. Instead of assuming that supervision can simply be scaled up, scalable oversight studies where and why supervision breaks down as model capability increases.

Pushing these questions further leads to Super Alignment, which addresses alignment in regimes where models surpass human ability to directly supervise their reasoning or outcomes. In these settings, alignment must rely on indirect objectives, weaker forms of oversight, or goals specified under deep uncertainty. This direction focuses on understanding the failure modes that emerge when human judgment is no longer a reliable reference point, and on clarifying what it even means for alignment to remain meaningful under such conditions. Super alignment is not treated as a distant or abstract problem, but as the limiting case implied by the trends observed in more practical alignment settings.

Seen together, these directions form a coherent progression. Robust alignment asks whether aligned behavior is stable outside training conditions. Evaluation and monitoring question whether we can reliably detect when that stability breaks. Scalable oversight examines whether alignment can be maintained as supervision weakens. Super alignment confronts the regime in which human judgment itself can no longer anchor alignment. Together, they reflect a shift from treating alignment as a collection of techniques to treating it as a scientific problem about stability, supervision, and epistemic reliability. This perspective continues to guide how I choose problems and design experiments, even as the specific methods evolve over time.
