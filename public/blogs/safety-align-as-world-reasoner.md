---
title: "Toward Generalized Safety Alignment: Your LLMs should align as World Reasoners"
date: "2025/12/28"
generator: "ChatGPT5.1"
---

*This blog was entirely generated by ChatGPT5.1 without any human revision. I only used some sentences to express my viewpoint, and I basically agree with the details of the article completion.*

In ethics, there exists a recurring dichotomy: whether morality derives from adherence to rules, or from understanding consequences. Kant believed that reason could establish action boundaries through duty itself, while consequentialists insist that moral judgment only truly holds when an agent can evaluate the cascading effects of its behavior in the world. This dichotomy appears abstract, yet it re-emerges in today's large language model safety alignment challenges with striking concreteness and sharpness.

Current mainstream large model safety alignment approaches more closely resemble an engineering implementation of "deontological ethics." Models are trained to recognize which inputs violate norms and which outputs should be prohibited, reproducing similar rejection or mitigation behaviors across analogous scenarios. From a technical pathway perspective, whether through SFT-based safety fine-tuning or RLHF/DPO incorporating human preferences and reward signals, these methods fundamentally still rely on pattern induction within post-training frameworks: as long as inputs are sufficiently close in form or semantics to known risk categories, the model should trigger corresponding safety responses.

The issue lies in the fact that this alignment approach does not require models to understand "why something is dangerous," nor does it demand internal representations of behavioral consequences. What models learn is a set of discriminative boundaries over linguistic forms, rather than a causal structure of world evolution. Consequently, this alignment is more accurately described as surface-level alignmentâ€”it appears stable and rational within-distribution, yet fails rapidly under even minor distribution shifts. This is precisely why jailbreaks succeed repeatedly across variants: the attacks are not challenging the model's values, but circumventing its pattern matcher.

In stark contrast stands human safety judgment. A mature human facing a potentially risky request typically does not react merely based on wording or form, but spontaneously engages in at least one-step-forward consequence reasoning: if I provide this information, what happens next? How will it be used? Will it convert to irreversible harm at some point? This reasoning is not always explicit, but it is deeply embedded in our understanding of the world. In other words, humans are not "executing rules," but continuously simulating the world.

![World Reasoner Illustration](blogs/images/world-reasoner.jpeg)
^Surface-level alignment enables models to reject explicit harmful requests, but fails under adversarial prompting; in contrast, world reasoning consistently evaluates real-world consequences and rejects both. Image generated by Nano Banana Pro.

This is precisely the core capability that current safety reasoning research universally lacks. Despite increasing work focusing on "safety reasoning" and attempting to enhance robustness through explicit reasoning chains, risk analysis steps, or staged responses, from a training paradigm perspective, these methods are largely just repackaging existing post-training pipelines. Models are required to output seemingly longer, more complex reasoning text, but these texts themselves are often just another form of imitation target, rather than externalization of the model's internal causal modeling capabilities. Models learn "how to write text that resembles safety reasoning," but are not compelled to genuinely care about the world states toward which the reasoning points.

A further issue lies in the fact that current safety SFT datasets exhibit significant distribution shifts and human design artifacts, consisting primarily of single-turn QA data. In such training processes, models more readily learn examination strategies: recognizing question types, reproducing templates, maximizing scores, rather than internalizing the deeper motivational structure of "why consequence reasoning matters." When models are deployed in open-ended, continuous, real-world interaction environments lacking explicit risk labels, this learning naturally fails to generalize.

Beyond this, the pure text world itself imposes fundamental limitations on safety alignment. Actions within text are revocable; consequences are described, not experienced; risks are semantic, not physical or social. It is within this low-cost, low-constraint environment that safety alignment readily degrades into a linguistic game, rather than constraint on real actions. This enables models to generate "reasonable risk analyses" while potentially not truly understanding what risk means.

Based on these observations, I believe the objective of safety alignment needs reframing. We should no longer merely demand that models become compliant rule followers, but progressively guide them to become world reasoners capable of consequence prediction and world modeling. Truly robust alignment is not about models memorizing which words cannot be spoken, but understanding: once certain behaviors are executed, how they will unfold in the world, and why they should not be promoted.

This also implies that existing safety reasoning paradigms have significant room for improvement. On one hand, reward design should not merely reward surface-level correctness, but encourage models to autonomously generate coherent, verifiable causal chains; on the other hand, training data should not be limited to single-turn QA, but incorporate more multi-turn interactions, allowing models to progressively expose and correct their consequence predictions during conversation. In such settings, safety becomes not a static label, but a dynamically evolving outcome.

From a longer-term perspective, embodied AI is undoubtedly a crucial direction for achieving human value alignment. Only when models truly interact with the physical world do consequences become non-ignorable and responsibility acquire real meaning. However, at the current stage, full embodied intelligence remains distant. A more practical and research-valuable path is constructing minimal world-interacting agents: enabling models to take actions in controlled yet feedback-rich environments, observe outcomes, and refine predictions, thereby gradually learning the world's causal structure.

Perhaps truly generalized safety alignment is not "infused" into models through more sophisticated rules, but "grown" incrementally through sustained world interaction.